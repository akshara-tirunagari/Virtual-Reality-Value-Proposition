{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a2ea4d-99b4-4883-ae2d-e7ffac4e46af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# V-Med Pro Analytics Demo Pipeline (Self-Contained Streamlit Version)\n",
    "File Name: analytics_app.py\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81748e2c-4a2e-4504-8b14-e647543d2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile analytics_app.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import spearmanr, ttest_ind\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for a clean demo\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "st.set_page_config(page_title=\"V-Med Pro Analytics Demo\", layout=\"wide\")\n",
    "st.title(\"VIZITECH‚ÄìASU Analytics Demo Pipeline üß†\")\n",
    "st.markdown(\"### Automated Data Cleaning ‚Üí Descriptive ‚Üí Hypothesis ‚Üí Predictive Flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f1fbcf-e5cb-4204-adfc-86ad9332bc5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. DATA GENERATION (Replaced Load)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec7b6e9-b20e-495b-83f2-2362a6078d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_rows=100):\n",
    "    \"\"\"Generates synthetic data mimicking survey responses.\"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    data = {\n",
    "        # General Survey Info\n",
    "        'Institution_Type': np.random.choice(['University', 'Community College', 'Vocational School'], n_rows),\n",
    "        'Region': np.random.choice(['North', 'South', 'East', 'West'], n_rows),\n",
    "        # Numeric Feature Columns (scaled 1-5 or 0-1)\n",
    "        'Q1_Infrastructure_Score': np.random.randint(1, 6, n_rows),\n",
    "        'Q2_Usability_Score': np.random.randint(1, 6, n_rows),\n",
    "        'Q3_Adoption_Experience': np.random.randint(1, 6, n_rows),\n",
    "        'Q4_Budget_Allocation': np.random.uniform(0, 1, n_rows),\n",
    "        'Q5_Training_Hours': np.random.uniform(10, 50, n_rows),\n",
    "        # Hypothesis & Prediction Target Columns\n",
    "        'Satisfaction': np.random.uniform(3, 5, n_rows),\n",
    "        'Likelihood_to_Adopt': np.random.uniform(0, 1, n_rows),\n",
    "        'Group': np.random.choice([0, 1], n_rows, p=[0.6, 0.4]) # 0=HEIs, 1=Professionals\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Introduce some realistic missing data for the cleaning step to work\n",
    "    for col in ['Q1_Infrastructure_Score', 'Q4_Budget_Allocation']:\n",
    "        df.loc[df.sample(frac=0.05).index, col] = np.nan\n",
    "    df.loc[df.sample(frac=0.03).index, 'Region'] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "st.subheader(\"üì• Step 1: Load/Generate Data\")\n",
    "\n",
    "try:\n",
    "    # Use synthetic data instead of file path\n",
    "    df = generate_synthetic_data(n_rows=200)\n",
    "    st.write(\"**Synthetic Survey Data (first few rows):**\")\n",
    "    st.dataframe(df.head())\n",
    "    st.info(\"üí° Data loaded from a synthetic generator for universal execution.\")\n",
    "except Exception as e:\n",
    "    st.error(f\"Error generating dataset: {e}\")\n",
    "    st.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe878ff-40b1-4eb8-808d-a23efb51f636",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. DATA CLEANING\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b289cde6-8614-4de4-8d82-ad4d892b1075",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.subheader(\"üßπ Step 2: Data Cleaning\")\n",
    "\n",
    "st.markdown(\"**Handling Missing Values (Mean / Median / Mode Imputation)**\")\n",
    "\n",
    "missing_before = df.isna().sum().sum()\n",
    "\n",
    "# Fill numeric with median, object with mode\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'O':  # object/text\n",
    "        if not df[col].mode().empty:\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(\"Unknown\", inplace=True)\n",
    "    else:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "missing_after = df.isna().sum().sum()\n",
    "\n",
    "col1, col2 = st.columns(2)\n",
    "col1.metric(\"Missing Values (Before)\", missing_before)\n",
    "col2.metric(\"Missing Values (After)\", missing_after)\n",
    "\n",
    "# Outlier handling (IQR method)\n",
    "st.markdown(\"**Outlier Detection (IQR Method)**\")\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "for col in numeric_cols:\n",
    "    Q1, Q3 = df[col].quantile(0.25), df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "    df[col] = np.clip(df[col], lower, upper)\n",
    "\n",
    "# Encoding categorical variables\n",
    "st.markdown(\"**Encoding Categorical Columns**\")\n",
    "label_enc = LabelEncoder()\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    # Ensure all values are strings before encoding\n",
    "    df[col] = label_enc.fit_transform(df[col].astype(str))\n",
    "\n",
    "# Normalize numeric data\n",
    "scaler = MinMaxScaler()\n",
    "# Normalize the entire dataframe, including newly encoded columns\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "st.success(\"‚úÖ Data cleaned, encoded, and normalized successfully!\")\n",
    "st.dataframe(df_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64307ed1-b4fb-4b2a-b0bb-32e00f3e092b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. DESCRIPTIVE ANALYTICS\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67eb0f2-3f84-45f6-be09-0721eb525f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.subheader(\"üìä Step 3: Descriptive Analytics\")\n",
    "\n",
    "st.markdown(\"**Central Tendency & Dispersion Statistics**\")\n",
    "st.dataframe(df_scaled.describe().T)\n",
    "\n",
    "#-----\n",
    "# Visualizations\n",
    "# -----\n",
    "st.markdown(\"**Visualizations (ASU Maroon & Gold Theme)**\")\n",
    "\n",
    "# Pick top 5 numeric columns for demo plotting\n",
    "numeric_cols_all = df_scaled.select_dtypes(include=np.number).columns.tolist()\n",
    "# Filter out target/group columns for general feature visualization\n",
    "feature_cols = [col for col in numeric_cols_all if col not in ['Satisfaction', 'Likelihood_to_Adopt', 'Group']][:5]\n",
    "\n",
    "if feature_cols:\n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Boxplot\n",
    "    # Using the first two columns for demonstration colors\n",
    "    sns.boxplot(data=df_scaled[feature_cols], ax=axes[0], palette=[\"#8C1D40\", \"#FFC627\", \"#8C1D40\", \"#FFC627\", \"#8C1D40\"])\n",
    "    axes[0].set_title(\"Boxplot: Outlier Spread (Top 5 Variables)\", fontsize=10)\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Histogram\n",
    "    df_scaled[feature_cols].plot(kind='hist', bins=10, alpha=0.7, ax=axes[1],\n",
    "                                 legend=False, color=\"#8C1D40\")\n",
    "    axes[1].set_title(\"Histogram: Frequency Distribution\", fontsize=10)\n",
    "    axes[1].set_xlabel(\"Scaled Values\")\n",
    "\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    # Optional: Interactive single variable view\n",
    "    st.markdown(\"**Interactive Variable Visualization**\")\n",
    "    selected_col = st.selectbox(\"Choose a numeric column to visualize:\", feature_cols)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    sns.boxplot(y=df_scaled[selected_col], color=\"#FFC627\", ax=ax[0])\n",
    "    ax[0].set_title(f\"Boxplot: {selected_col}\")\n",
    "\n",
    "    sns.histplot(df_scaled[selected_col], kde=True, color=\"#8C1D40\", ax=ax[1])\n",
    "    ax[1].set_title(f\"Histogram: {selected_col}\")\n",
    "\n",
    "    st.pyplot(fig)\n",
    "else:\n",
    "    st.warning(\"Not enough numeric columns available for visualization after encoding.\")\n",
    "\n",
    "# Correlation heatmap\n",
    "st.markdown(\"**Correlation Heatmap**\")\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# Ensure columns exist before calculating correlation\n",
    "if not df_scaled.empty:\n",
    "    sns.heatmap(df_scaled.corr(), annot=True, cmap=\"YlOrRd\", fmt=\".2f\", ax=ax)\n",
    "    ax.set_title(\"Feature Correlation Matrix\")\n",
    "    st.pyplot(fig)\n",
    "else:\n",
    "    st.warning(\"Cannot generate heatmap, scaled data is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3ebd5-1e2d-4d35-8b70-8d927fa1bee6",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# 4. HYPOTHESIS TESTING\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655c5d2-b3d6-4146-8b95-61a915a33f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.subheader(\"üìà Step 4: Hypothesis Testing (Demo)\")\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "**H‚ÇÅ:** Satisfaction ‚Üî Likelihood_to_Adopt (Spearman Correlation)  \n",
    "**H‚ÇÇ:** HEIs vs Professionals (Independent t-Test on Adoption Likelihood)\n",
    "\"\"\")\n",
    "\n",
    "# Check for existence of required columns\n",
    "required_cols = ['Satisfaction', 'Likelihood_to_Adopt', 'Group']\n",
    "if all(col in df_scaled.columns for col in required_cols):\n",
    "    # Spearman correlation\n",
    "    corr, p_corr = spearmanr(df_scaled['Satisfaction'], df_scaled['Likelihood_to_Adopt'])\n",
    "    st.write(f\"**Spearman Correlation (H‚ÇÅ):** œÅ = {corr:.3f}, p = {p_corr:.3f}\")\n",
    "\n",
    "    # t-Test (HEI vs Professional)\n",
    "    group0 = df_scaled[df_scaled['Group'] == 0]['Likelihood_to_Adopt']\n",
    "    group1 = df_scaled[df_scaled['Group'] == 1]['Likelihood_to_Adopt']\n",
    "\n",
    "    if len(group0) > 1 and len(group1) > 1:\n",
    "        tstat, pval = ttest_ind(group0, group1)\n",
    "        # Corrected f-string formatting\n",
    "        st.write(f\"**t-Test (H‚ÇÇ):** t = {tstat:.3f}, p = {pval:.3f}\")\n",
    "\n",
    "        if pval < 0.05:\n",
    "            st.success(\"‚úÖ Statistically significant difference detected (p < 0.05).\")\n",
    "        else:\n",
    "            st.warning(\"‚ö†Ô∏è No significant difference detected (p ‚â• 0.05).\")\n",
    "    else:\n",
    "        st.warning(\"‚ö†Ô∏è Insufficient data in both groups (HEIs and Professionals) for t-Test.\")\n",
    "else:\n",
    "    st.error(f\"Hypothesis testing skipped. Required columns missing: {required_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2202999-4ea5-45be-8d01-e23af40c7964",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# 5. PREDICTIVE MODEL (Demo)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b35fa0-0bb0-43da-bc48-5f544635499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.subheader(\"ü§ñ Step 5: Predictive Modeling (Demo)\")\n",
    "\n",
    "# Define target and features\n",
    "target_col = 'Likelihood_to_Adopt'\n",
    "\n",
    "if target_col in df_scaled.columns and len(df_scaled) > 2:\n",
    "    # Create binary target variable (High Adoption Likelihood vs Low)\n",
    "    y = (df_scaled[target_col] > df_scaled[target_col].median()).astype(int)\n",
    "\n",
    "    # Features: Drop the target and constant/near-constant columns if any were created during scaling\n",
    "    X = df_scaled.drop(columns=[target_col], axis=1, errors='ignore')\n",
    "\n",
    "    # Drop columns that are constant (standard for logistic regression)\n",
    "    X = X.loc[:, (X != X.iloc[0]).any()]\n",
    "\n",
    "    if X.shape[1] > 0 and len(X) >= 2:\n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # Train Logistic Regression Model\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict probabilities on test set\n",
    "        proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Display results\n",
    "        st.write(\"**Predicted High Adoption Likelihood Probabilities (Test Set):**\")\n",
    "        prediction_df = pd.DataFrame({\n",
    "            \"Test_Sample_Index\": X_test.index,\n",
    "            \"Predicted_Prob\": np.round(proba, 3)\n",
    "        }).set_index(\"Test_Sample_Index\")\n",
    "        st.dataframe(prediction_df.head(10))\n",
    "\n",
    "        # Bar chart visualization\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "        ax.bar(prediction_df.index[:20], prediction_df['Predicted_Prob'][:20], color='#8C1D40')\n",
    "        ax.set_title(\"Predicted Adoption Probability for First 20 Test Samples\")\n",
    "        ax.set_ylabel(\"Probability\")\n",
    "        ax.set_xlabel(\"Test Sample Index\")\n",
    "        st.pyplot(fig)\n",
    "        st.success(\"‚úÖ Predictive model executed successfully!\")\n",
    "    else:\n",
    "        st.warning(\"‚ö†Ô∏è Not enough unique features or data points to train the predictive model.\")\n",
    "else:\n",
    "    st.warning(\"‚ö†Ô∏è Target column 'Likelihood_to_Adopt' is missing or dataset is too small to train the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe7c8f4-eb82-4dd9-a584-2ba57f457e9b",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# 6. PIPELINE SUMMARY\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d40ed52-b92b-4e47-848b-c1d92afe3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.subheader(\"üöÄ End-to-End Pipeline Completed\")\n",
    "st.markdown(\"\"\"\n",
    "This demo shows an **automated, modular pipeline** covering:\n",
    "- Data Generation/Loading ‚Üí Cleaning ‚Üí Descriptive ‚Üí Hypothesis ‚Üí Predictive  \n",
    "- Designed for continuous integration of new data (HEIs + Professionals).  \n",
    "- Fully scalable for Power BI / Airflow integration in production.\n",
    "\"\"\")\n",
    "\n",
    "st.balloons()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957c7bb8-ecf6-4a94-8520-8b201f6b6800",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Running on terminal\n",
    "\n",
    "pip install streamlit\n",
    "\n",
    "cd Downloads\n",
    "streamlit run analytics_app.py\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb4e385-29ac-4f91-9f59-ec52c9ae786d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
